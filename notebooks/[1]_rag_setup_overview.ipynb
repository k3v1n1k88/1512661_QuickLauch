{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
      "metadata": {
        "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe"
      },
      "source": [
        "# bRAG: Introduction to Retrieval-Augmented Generation (RAG) Setup\n",
        "\n",
        "These notebooks walk through the process of building RAG app(s) from scratch.\n",
        "\n",
        "They will build towards a broader understanding of the RAG langscape, as shown here:\n",
        "\n",
        "![Screenshot 2024-03-25 at 8.30.33 PM.png](attachment:c566957c-a8ef-41a9-9b78-e089d35cf0b7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f1e8928",
      "metadata": {
        "id": "8f1e8928"
      },
      "source": [
        "## Pre-requisites (optional but recommended)\n",
        "\n",
        "### Only do the first step if you have never created a virtual environment for this repository. Otherwise, make sure that the Python Kernel that you selected is from your `venv/` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "706c950a",
      "metadata": {
        "id": "706c950a",
        "outputId": "9603f006-3010-451a-dbf5-47e8104c51f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['/venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n"
          ]
        }
      ],
      "source": [
        "# Create virtual environment\n",
        "! python -m venv ../venv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
      "metadata": {
        "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a"
      },
      "outputs": [],
      "source": [
        "# Activate virtual Python environment\n",
        "! source ../venv/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5b208d",
      "metadata": {
        "id": "5d5b208d",
        "outputId": "0f96c61e-92a8-4a3f-959b-061628edec5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "# If your Python is not from your venv path, ensure that your IDE's kernel selection (on the top right corner) is set to the correct path\n",
        "# (your path output should contain \"...venv/bin/python\")\n",
        "\n",
        "! which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23676380",
      "metadata": {
        "id": "23676380"
      },
      "outputs": [],
      "source": [
        "# Install all packages\n",
        "! pip install -r ../requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74b1fb5",
      "metadata": {
        "id": "b74b1fb5"
      },
      "source": [
        "### * If you choose to skip the pre-requisites and install only the packages specific to this notebook using your global Python path environment, execute the command below; otherwise, proceed to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5882e8b",
      "metadata": {
        "id": "d5882e8b"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet pinecone-client python-dotenv langchain langchain-community langchain-core langchain-openai beautifulsoup4 tiktoken numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b0865a",
      "metadata": {
        "id": "a5b0865a"
      },
      "source": [
        "## Environment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea23028",
      "metadata": {
        "id": "8ea23028"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load all environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the environment variables\n",
        "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
        "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
        "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
        "\n",
        "## LLM\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "## Pinecone Vector Database\n",
        "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
        "pinecone_api_host = os.getenv('PINECONE_API_HOST')\n",
        "index_name = os.getenv('PINECONE_INDEX_NAME')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a8ab66-8477-429f-bbbe-ba439322d085",
      "metadata": {
        "id": "75a8ab66-8477-429f-bbbe-ba439322d085"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76f68a8-4745-4377-8057-6090b87377d1",
      "metadata": {
        "id": "b76f68a8-4745-4377-8057-6090b87377d1"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
        "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8eb312d-8a07-4df3-8462-72ac526715f7",
      "metadata": {
        "id": "f8eb312d-8a07-4df3-8462-72ac526715f7"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
      "metadata": {
        "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "#Pinecone keys\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "os.environ['PINECONE_API_HOST'] = pinecone_api_host\n",
        "os.environ['PINECONE_INDEX_NAME'] = index_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6QX53ltopgu"
      },
      "source": [
        "`(4) Pinecone Init`"
      ],
      "id": "G6QX53ltopgu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950dbf51",
      "metadata": {
        "id": "950dbf51",
        "outputId": "b5e95a13-3308-4b60-c36f-fa432b3137b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
        "index = pc.Index(os.environ['PINECONE_INDEX_NAME'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
      "metadata": {
        "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c"
      },
      "source": [
        "## Part 1: Overview\n",
        "\n",
        "[RAG quickstart](https://python.langchain.com/docs/tutorials/rag/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
      "metadata": {
        "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
        "outputId": "8ff89e77-f10d-4b64-dd9e-20cc699f3e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('LangChain uses vector stores to embed and store diverse data sources like '\n",
            " 'documents, text, and images. When a user submits a query, the system '\n",
            " 'retrieves the most relevant information from the vector store based on '\n",
            " 'vector similarity. This retrieved context is then provided to the large '\n",
            " 'language model (LLM) to enhance its ability to generate accurate responses.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "import bs4\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "vectorstore = Pinecone.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "pprint(rag_chain.invoke(\"How does LangChain use vector stores for efficient data retrieval?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e8e856-bafd-469e-b99a-11596b18aad4",
      "metadata": {
        "id": "18e8e856-bafd-469e-b99a-11596b18aad4"
      },
      "source": [
        "## Part 2: Indexing\n",
        "\n",
        "![Screenshot 2024-02-12 at 1.36.56 PM.png](attachment:d1c0f19e-1f5f-4fc6-a860-16337c1910fa.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16",
      "metadata": {
        "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16"
      },
      "outputs": [],
      "source": [
        "# Documents\n",
        "question = \"How does LangChain handle multi-turn conversations in chat models?\"\n",
        "document = \"LangChain supports chat models that manage complex, multi-turn conversations by maintaining state across interaction turns. This is achieved through structured message handling and conversation memory, which allows for the retention of historical context. Features like structured outputs and JSON-based response formatting further enable seamless integration with downstream applications, making it ideal for context-dependent use cases such as customer support and conversational AI.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304",
      "metadata": {
        "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304"
      },
      "source": [
        "[Count tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considering [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df119cca-1676-4caa-bad4-11805d69e616",
      "metadata": {
        "id": "df119cca-1676-4caa-bad4-11805d69e616",
        "outputId": "6bcdd643-4479-4561-8c1f-8abe70fa3cb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f",
      "metadata": {
        "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f"
      },
      "source": [
        "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
      "metadata": {
        "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
        "outputId": "84f6db83-7878-4ad4-f0fc-29cc9f601e34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embd = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8",
      "metadata": {
        "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8"
      },
      "source": [
        "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical) for OpenAI embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8001998-b08c-4560-b124-bfa1fced8958",
      "metadata": {
        "id": "b8001998-b08c-4560-b124-bfa1fced8958",
        "outputId": "ea0dc548-3406-4885-ec35-0397a4ab8173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.7714784751249568\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20",
      "metadata": {
        "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20"
      },
      "source": [
        "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n",
        "\n",
        "Includes file loaders for:\n",
        "- [Webpages](https://python.langchain.com/docs/integrations/document_loaders/#webpages)\n",
        "- [PDFs](https://python.langchain.com/docs/integrations/document_loaders/#pdfs)\n",
        "- [Cloud Providers](https://python.langchain.com/docs/integrations/document_loaders/#cloud-providers)\n",
        "- [Social Platforms](https://python.langchain.com/docs/integrations/document_loaders/#social-platforms)\n",
        "- [Messaging Services](https://python.langchain.com/docs/integrations/document_loaders/#messaging-services)\n",
        "- [Productivity tools](https://python.langchain.com/docs/integrations/document_loaders/#productivity-tools)\n",
        "- [Common File Types](https://python.langchain.com/docs/integrations/document_loaders/#common-file-types)\n",
        "  - CSVLoader: CSV files\n",
        "  - DirectoryLoader: All files in a given directory\n",
        "  - Unstructured: Many file types (see https://docs.unstructured.io/platform/supported-file-types)\n",
        "  - JSONLoader: JSON files\n",
        "  - BSHTMLLoader: HTML filess\n",
        "- All document loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
      "metadata": {
        "id": "5778c31a-6138-4130-8865-31a08e82b9fb"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e731e-c6ff-46e3-a8bc-386832362af2",
      "metadata": {
        "id": "798e731e-c6ff-46e3-a8bc-386832362af2"
      },
      "source": [
        "[Splitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n",
        "\n",
        "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e668d339-3951-4662-8387-c3d296646906",
      "metadata": {
        "id": "e668d339-3951-4662-8387-c3d296646906"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d",
      "metadata": {
        "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d"
      },
      "source": [
        "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)\n",
        "\n",
        "A vector store stores embedded data and performs similarity search.\n",
        "\n",
        "50+ vectorstores available to choose from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41",
      "metadata": {
        "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "\n",
        "vectorstore = Pinecone.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba890329-1411-4922-bd27-fe0490dd1208",
      "metadata": {
        "id": "ba890329-1411-4922-bd27-fe0490dd1208"
      },
      "source": [
        "## Part 3: Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
      "metadata": {
        "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "\n",
        "vectorstore = Pinecone.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda",
      "metadata": {
        "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda"
      },
      "outputs": [],
      "source": [
        "docs = retriever.invoke(\"How does LangChain handle multi-turn conversations in chat models?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db96f877-60d3-4741-9846-e2903831583d",
      "metadata": {
        "id": "db96f877-60d3-4741-9846-e2903831583d",
        "outputId": "b148802a-2c04-4257-a86b-4564b50edfd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
      "metadata": {
        "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2"
      },
      "source": [
        "## Part 4: Generation\n",
        "\n",
        "![Screenshot 2024-02-12 at 1.37.38 PM.png](attachment:f9b0e284-58e4-4d33-9594-2dad351c569a.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
      "metadata": {
        "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
        "outputId": "c34a6298-e2d5-4e58-b6fb-a9928ba5ecb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4461264-5cac-479a-917c-9bf589826da4",
      "metadata": {
        "id": "e4461264-5cac-479a-917c-9bf589826da4"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
      "metadata": {
        "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d"
      },
      "outputs": [],
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
      "metadata": {
        "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
        "outputId": "38a0dc1e-314c-4119-cd6d-1c4cc4c22cb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain maintains state across conversation turns, making it suitable for prolonged, context-dependent conversations.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 495, 'total_tokens': 514, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fd45bacc-e13a-4263-b397-20ad587bfa02-0', usage_metadata={'input_tokens': 495, 'output_tokens': 19, 'total_tokens': 514, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":docs,\"question\":\"How does LangChain handle multi-turn conversations in chat models?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65770e2d-3d5e-4371-abc9-0aeca9646885",
      "metadata": {
        "id": "65770e2d-3d5e-4371-abc9-0aeca9646885"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
      "metadata": {
        "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
        "outputId": "57d44a27-6bac-4b86-9bfb-d36c89c3df2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_hub_rag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffe29a1-5527-419e-9f12-8a3061d12885",
      "metadata": {
        "id": "8ffe29a1-5527-419e-9f12-8a3061d12885"
      },
      "source": [
        "[RAG chains](https://python.langchain.com/docs/how_to/sequence/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
      "metadata": {
        "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
        "outputId": "d34fe828-30c9-445e-8b00-601d6baad624"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChain enhances LLM responses with Retrieval-Augmented Generation (RAG) by allowing models to access up-to-date information through Document Loaders, Text Splitters, Embedding Models, Vector Stores, Retrievers, and RAG Chains. This enables the retrieval and merging of external data with model responses, enhancing applications such as question answering systems and recommendation engines.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"How does LangChain enhance LLM responses with Retrieval-Augmented Generation (RAG)?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa245709",
      "metadata": {
        "id": "aa245709"
      },
      "source": [
        "# Conclusion: Basic RAG Setup\n",
        "\n",
        "This document provides an overview of the basic setup for a Retrieval-Augmented Generation (RAG) application. It outlines the necessary steps to create a virtual environment, install required packages, and load environment variables for API keys.\n",
        "\n",
        "Key components include:\n",
        "\n",
        "1. **Environment Setup**: Instructions for creating a virtual environment and installing dependencies from a `requirements.txt` file.\n",
        "2. **API Key Management**: Loading and setting environment variables for various services, including LangChain and Pinecone.\n",
        "3. **Document Loading and Indexing**: Utilizing `WebBaseLoader` to load documents from a specified URL, followed by splitting the documents into manageable chunks using `RecursiveCharacterTextSplitter`.\n",
        "4. **Embedding and Vector Store**: Creating embeddings for the documents and storing them in a Pinecone vector database for efficient retrieval.\n",
        "5. **Retrieval and Generation**: Implementing a retrieval mechanism to fetch relevant documents based on user queries and generating responses using a language model (LLM).\n",
        "\n",
        "This setup serves as a foundational framework for building RAG applications. Future notebooks will go deeper into each component, providing more advanced techniques and use cases for leveraging RAG effectively."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}